{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d6296d-9187-4737-ab9b-b73bc0d521bc",
   "metadata": {},
   "source": [
    "# **Final Presentation**\n",
    "\n",
    "\n",
    "\n",
    "In this notebook team Power_Factor will describe its developed package: the different functionalities, the completed assignments that led to the developement of the package as well as some code demonstrations and collaboration discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b52833-9e5e-40c8-a268-43a2bc0c35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_array\n",
    "from IPython.display import display\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from power_grid_model.utils import json_deserialize, json_serialize\n",
    "from power_grid_model.validation import ValidationException, assert_valid_batch_data, assert_valid_input_data\n",
    "from power_grid_model import (\n",
    "    CalculationMethod,\n",
    "    CalculationType,\n",
    "    ComponentType,\n",
    "    DatasetType,\n",
    "    PowerGridModel,\n",
    "    initialize_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8859b97",
   "metadata": {},
   "source": [
    "# **Assignment 1 -graph processing**\n",
    "\n",
    "In assignment 1 the task is to build a graph processing class. For a given undirected graph as input, there are two functionalities to implement:\n",
    "- Find downstream vertices - given a specific edge from the graph, find the downstream vertices of the edge, including the downstream vertex of the edge itself\n",
    "- Find alternative edges - given a specific enabled edge from the graph, returns a list that indicates which currently disabled edges can be enabled so that the graph is again fully connected and acyclic\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742795ef",
   "metadata": {},
   "source": [
    "# **Check input validity**\n",
    "\n",
    "Before the either of the two functionalities are used, the input data must be checked for validity. There are 7 criteria that are evaluated:\n",
    "1. The vertex (node) and edge ids should be unique\n",
    "2. The number of edges (or connections between two separate vertices) should have the same as the number of edge ids\n",
    "3. The vertices connected by the specified edges should be valid vertex ids\n",
    "4. The number of enabled/disabled edges should be the same as the number of edge ids\n",
    "5. The id of the source vertex should be a valid vertex id\n",
    "6. The graph should be fully connected\n",
    "7. The graph should not contain cycles\n",
    "\n",
    "Furthermore, find_alternative_edges functionality is important to check if the edge to be disabled has a valid id and whether it is already disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "787f2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDNotFoundError(Exception):\n",
    "    \"Raised when a source or edge id is not found/valid\"\n",
    "\n",
    "\n",
    "class InputLengthDoesNotMatchError(Exception):\n",
    "    \"Raised when number of enabled and disabled edges does not match number of total edges or number of vertex pairs does not match number of edges\"\n",
    "\n",
    "\n",
    "class IDNotUniqueError(Exception):\n",
    "    \"Raised when vertex or edge ids are not unique\"\n",
    "\n",
    "\n",
    "class GraphNotFullyConnectedError(Exception):\n",
    "    \"Raised when graph contains more than 1 component\"\n",
    "\n",
    "\n",
    "class GraphCycleError(Exception):\n",
    "    \"Raised when graph contains a cycle\"\n",
    "\n",
    "\n",
    "class EdgeAlreadyDisabledError(Exception):\n",
    "    \"Edge is already disabled\"\n",
    "\n",
    "\n",
    "def check_unique(vertex_ids, edge_ids):  # checks vertex ids or edge ids are unique\n",
    "    ver = list(set(vertex_ids))  # a set is used because sets contain only unique values\n",
    "    edge = list(set(edge_ids))\n",
    "    if len(ver) != len(vertex_ids) or len(edge) != len(edge_ids):\n",
    "        raise IDNotUniqueError(\"Vertex or edge ids are not unique\")\n",
    "\n",
    "\n",
    "def check_length_pairs(edge_vertex_id_pairs, edge_ids):  # checks if number of vertex pairs matches the number of edges\n",
    "    if len(edge_vertex_id_pairs) != len(edge_ids):\n",
    "        raise InputLengthDoesNotMatchError(\"Number of vertex pairs does not match number of edges\")\n",
    "\n",
    "\n",
    "def check_found_pairs(edge_vertex_id_pairs, vertex_ids):  # checks if all vertex pairs contain valid vertex ids\n",
    "    if all(all(elem in vertex_ids for elem in t) for t in edge_vertex_id_pairs) == False:\n",
    "        raise IDNotFoundError(\"Vertex id not found in edge array\")\n",
    "\n",
    "\n",
    "def check_length_enabled(\n",
    "    edge_enabled, edge_ids\n",
    "):  # checks if the number of enabled/disabled edges is the same as the number of edge ids\n",
    "    if len(edge_enabled) != len(edge_ids):\n",
    "        raise InputLengthDoesNotMatchError(\"Number of enabled and disabled edges does not match number of total edges\")\n",
    "\n",
    "\n",
    "def check_found_source(source_vertex_id, vertex_ids):  # checks if the source vertex id is valid\n",
    "    if source_vertex_id not in vertex_ids:\n",
    "        raise IDNotFoundError(\"Source vertex id not found\")\n",
    "\n",
    "\n",
    "def check_connect(vertex_ids, edge_enabled, edge_vertex_id_pairs):  # checks if graph is fully connected\n",
    "    size = len(vertex_ids)\n",
    "    sparseMatrix = [[0 for i in range(size)] for j in range(size)]\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if ((vertex_ids[i], vertex_ids[j]) in edge_vertex_id_pairs) and sparseMatrix[i][j] == 0:\n",
    "                if edge_enabled[edge_vertex_id_pairs.index((vertex_ids[i], vertex_ids[j]))]:\n",
    "                    sparseMatrix[i][j] = vertex_ids[j]\n",
    "                    sparseMatrix[j][i] = vertex_ids[i]\n",
    "            elif ((vertex_ids[j], vertex_ids[i]) in edge_vertex_id_pairs) and sparseMatrix[i][j] == 0:\n",
    "                if edge_enabled[edge_vertex_id_pairs.index((vertex_ids[j], vertex_ids[i]))]:\n",
    "                    sparseMatrix[i][j] = vertex_ids[j]\n",
    "                    sparseMatrix[j][i] = vertex_ids[i]\n",
    "    graph = csr_array(sparseMatrix)\n",
    "    components = connected_components(graph)\n",
    "    if components[0] > 1:\n",
    "        raise GraphNotFullyConnectedError(\"Graph contains more than 1 component\")\n",
    "\n",
    "\n",
    "def check_cycle(edge_enabled, edge_vertex_id_pairs):  # checks if graph contains cycles\n",
    "    G = nx.Graph()\n",
    "    for (u, v), enabled in zip(edge_vertex_id_pairs, edge_enabled):\n",
    "        if enabled:\n",
    "            G.add_edge(u, v)\n",
    "\n",
    "    has_cycle = nx.is_forest(G)  # A forest is a graph with no undirected cycles\n",
    "    if has_cycle == False:\n",
    "        raise GraphCycleError(\"Graph contains a cycle\")\n",
    "\n",
    "\n",
    "def check_found_edges(disabled_edge_id, all_edges):  # checks if the edge to be disabled has a valid edge id\n",
    "    if disabled_edge_id not in all_edges:\n",
    "        raise IDNotFoundError(\"Disabled edge id not found in edge array\")\n",
    "\n",
    "\n",
    "def check_disabled(disabled_edge_id, edge_ids, edge_enabled):  # checks if the edge to be disabled is already disabled\n",
    "    if edge_enabled[edge_ids.index(disabled_edge_id)] == False:\n",
    "        raise EdgeAlreadyDisabledError(\"Edge is already disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb58586",
   "metadata": {},
   "source": [
    "With all of the necessary needed external functions declared, the graph processing class can now also be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42465a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphProcessor(nx.Graph):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vertex_ids: List[int],\n",
    "        edge_ids: List[int],\n",
    "        edge_vertex_id_pairs: List[Tuple[int, int]],\n",
    "        edge_enabled: List[bool],\n",
    "        source_vertex_id: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        check_unique(vertex_ids, edge_ids)\n",
    "        check_length_pairs(edge_vertex_id_pairs, edge_ids)\n",
    "        check_found_pairs(edge_vertex_id_pairs, vertex_ids)\n",
    "        check_length_enabled(edge_enabled, edge_ids)\n",
    "        check_found_source(source_vertex_id, vertex_ids)\n",
    "        check_connect(vertex_ids, edge_enabled, edge_vertex_id_pairs)\n",
    "        check_cycle(edge_enabled, edge_vertex_id_pairs)\n",
    "\n",
    "        self.source_vertex_id = source_vertex_id\n",
    "        self.vertex_ids = vertex_ids\n",
    "        self.edge_enabled = edge_enabled\n",
    "        self.edge_ids = edge_ids\n",
    "        self.edge_vertex_id_pairs = edge_vertex_id_pairs\n",
    "        self.add_nodes_from(vertex_ids)\n",
    "        for i, (u, v) in enumerate(edge_vertex_id_pairs):\n",
    "            self.add_edge(u, v, id=edge_ids[i], enabled=edge_enabled[i])\n",
    "\n",
    "        self.enabled_subgraph = nx.Graph()\n",
    "        for (u, v), enabled in zip(edge_vertex_id_pairs, edge_enabled):\n",
    "            if enabled:\n",
    "                self.enabled_subgraph.add_edge(u, v)\n",
    "        # DFS tree & parent map from source\n",
    "        self.dfs_tree = nx.dfs_tree(self.enabled_subgraph, self.source_vertex_id)\n",
    "        self.parent_map = {child: parent for parent, child in nx.dfs_edges(self.dfs_tree, source=self.source_vertex_id)}\n",
    "\n",
    "    def find_downstream_vertices(self, edge_id: int) -> List[int]:\n",
    "        if edge_id not in self.edge_ids:\n",
    "            raise IDNotFoundError(\"Edge ID not found.\")\n",
    "\n",
    "        edge_index = self.edge_ids.index(edge_id)\n",
    "        if not self.edge_enabled[edge_index]:\n",
    "            return []\n",
    "\n",
    "        u, v = self.edge_vertex_id_pairs[edge_index]\n",
    "\n",
    "        # Ensure both u and v are reachable\n",
    "        if u not in self.dfs_tree or v not in self.dfs_tree:\n",
    "            return []\n",
    "\n",
    "        # Determine downstream vertex (child in DFS tree)\n",
    "        if self.parent_map.get(v) == u:\n",
    "            downstream_root = v\n",
    "        elif self.parent_map.get(u) == v:\n",
    "            downstream_root = u\n",
    "        else:\n",
    "            # If neither is parent of the other, one of them is ancestor; pick the child\n",
    "            # or fallback to whichever is deeper\n",
    "            depth = nx.single_source_shortest_path_length(self.dfs_tree, self.source_vertex_id)\n",
    "            downstream_root = v if depth.get(v, 0) > depth.get(u, 0) else u\n",
    "\n",
    "        descendants = list(nx.descendants(self.dfs_tree, downstream_root))\n",
    "        return [downstream_root] + descendants\n",
    "\n",
    "    def find_alternative_edges(self, disabled_edge_id: int) -> List[int]:\n",
    "        ans = []\n",
    "        check_found_edges(disabled_edge_id, self.edge_ids)\n",
    "        check_disabled(disabled_edge_id, self.edge_ids, self.edge_enabled)\n",
    "        H = nx.Graph()\n",
    "        for i, (u, v) in enumerate(self.edge_vertex_id_pairs):\n",
    "            if self.edge_enabled[i] == True and self.edge_ids[i] != disabled_edge_id:\n",
    "                H.add_edge(u, v)\n",
    "        for i, (u, v) in enumerate(self.edge_vertex_id_pairs):\n",
    "            if self.edge_enabled[i] == False and self.edge_ids[i] != disabled_edge_id:\n",
    "                H.add_edge(u, v)\n",
    "                if nx.number_connected_components(H) == 1 and nx.is_forest(H):\n",
    "                    ans.append(self.edge_ids[i])\n",
    "                H.remove_edge(u, v)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03aaf57",
   "metadata": {},
   "source": [
    "With the class implemented, its functionalities can now be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55e5ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of alternative edges when disabling edge 3 is: [7, 8]\n"
     ]
    }
   ],
   "source": [
    "vertex_ids1 = [0, 2, 4, 6, 10]\n",
    "edge_ids1 = [1, 3, 5, 7, 9, 8]\n",
    "edge_vertex_id_pairs1 = [(0, 2), (0, 4), (0, 6), (2, 4), (2, 10), (4, 6)]\n",
    "edge_enabled1 = [True, True, True, False, True, False]\n",
    "source_vertex_id1 = 0\n",
    "\n",
    "\"\"\"\n",
    "vertex_0 (source) --edge_1(enabled)-- vertex_2 --edge_9(enabled)-- vertex_10\n",
    "|                               |\n",
    "|                           edge_7(disabled)\n",
    "|                               |\n",
    "-----------edge_3(enabled)-- vertex_4\n",
    "|                               |\n",
    "|                           edge_8(disabled)\n",
    "|                               |\n",
    "-----------edge_5(enabled)-- vertex_6\n",
    "\"\"\"\n",
    "\n",
    "test_graph1 = GraphProcessor(vertex_ids1, edge_ids1, edge_vertex_id_pairs1, edge_enabled1, source_vertex_id1)\n",
    "edge_to_disable = 3\n",
    "print(\n",
    "    f\"The list of alternative edges when disabling edge {edge_to_disable} is: {test_graph1.find_alternative_edges(edge_to_disable)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "285c1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of downstream vertices for edge 3 is [4, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "vertex_ids2 = [0, 2, 4, 6, 8, 10, 12]\n",
    "edge_ids2 = [1, 3, 5, 7, 9, 11]\n",
    "edge_vertex_id_pairs2 = [(0, 2), (2, 4), (2, 6), (4, 8), (8, 10), (6, 12)]\n",
    "edge_enabled2 = [True, True, True, True, True, True]\n",
    "source_vertex_id2 = 0\n",
    "\n",
    "\"\"\"\n",
    "    vertex_0 (source) --edge_1-- vertex_2 --edge_3-- vertex_4--edge 7--vertex 8 --edge 9--vertex 10\n",
    "                                    |\n",
    "                                  edge 5\n",
    "                                    |\n",
    "                                 vertex 6  --edge 11 --vertex 12\n",
    "\"\"\"\n",
    "\n",
    "test_graph2 = GraphProcessor(vertex_ids2, edge_ids2, edge_vertex_id_pairs2, edge_enabled2, source_vertex_id2)\n",
    "edge_down = 3\n",
    "print(f\"The list of downstream vertices for edge {edge_down} is {test_graph2.find_downstream_vertices(edge_down)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e64a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7abf7c",
   "metadata": {},
   "source": [
    "# **Assignment 2 - power grid model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cb1d4",
   "metadata": {},
   "source": [
    "For assignment 2, the first thing that needs to be done is to load the input data and batch profiles. If any of these are not valid a suitable exception is raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ceeb44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_data(filepath: str):\n",
    "    \"\"\"Read and deserialize input network data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath) as fp:\n",
    "            try:\n",
    "                data = fp.read()\n",
    "            except Exception:\n",
    "                raise ValidationException(\"Error reading input data file\")\n",
    "    except Exception:\n",
    "        raise ValidationException(\"Error reading input data file\")\n",
    "    try:\n",
    "        input_data = json_deserialize(data)\n",
    "    except Exception:\n",
    "        raise ValidationException(\"Error deserializing input data\")\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def load_batch_profiles(active_profile_path: str, reactive_profile_path: str):\n",
    "    \"\"\"\n",
    "    Read active and reactive power profiles from parquet files,\n",
    "    check matching timestamps, and return them.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        active_power_profile = pd.read_parquet(active_profile_path)\n",
    "        reactive_power_profile = pd.read_parquet(reactive_profile_path)\n",
    "    except Exception as e:\n",
    "        raise ValidationException(f\"Error reading batch profile files: {e}\")\n",
    "    if not active_power_profile.index.equals(reactive_power_profile.index):\n",
    "        raise ValidationException(\"Active and reactive batch data have different timestamps\")\n",
    "    return active_power_profile, reactive_power_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5380c6",
   "metadata": {},
   "source": [
    "Then, the batch profiles can be loaded which will become the update data for the power flow calculations. The user can choose which profile to load by passing a string.\n",
    "The transformer data is used for assignment 3, however, if nothing about the transformer is specified, the function will simply not create update data for the transformer, therefore making this function compatible with previous implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23751788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_update_data(\n",
    "    active_batch_profile: pd.DataFrame,\n",
    "    reactive_batch_profile: pd.DataFrame,\n",
    "    tap_pos: int = -1,\n",
    "    transformer_id: int = -1,\n",
    "    profile_type: str = \"active\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - active_batch_profile: DataFrame indexed by timestamps, columns=house IDs → active p\n",
    "      - reactive_batch_profile: same shape → reactive q\n",
    "      - transformer_id: the int ID of the MV/LV transformer\n",
    "      - tap_pos: the integer tap position to use for every time step\n",
    "    Build and return the full update_data dict for:\n",
    "      1) sym_load (p_specified, q_specified)\n",
    "      2) transformer (id, tap_pos, from_status, to_status)\n",
    "    \"\"\"\n",
    "    # initialize the sym_load update\n",
    "    load_profile = initialize_array(DatasetType.update, ComponentType.sym_load, active_batch_profile.shape)\n",
    "\n",
    "    ids = active_batch_profile.columns.to_numpy()\n",
    "    try:\n",
    "        load_profile[\"id\"] = ids\n",
    "    except (ValueError, TypeError):\n",
    "        load_profile[\"id\"] = np.array(ids, dtype=object)\n",
    "\n",
    "    # fill p_specified / q_specified\n",
    "    data_active = active_batch_profile.to_numpy()\n",
    "    data_reactive = reactive_batch_profile.to_numpy()\n",
    "    if profile_type == \"active\":\n",
    "        load_profile[\"p_specified\"] = data_active\n",
    "        load_profile[\"q_specified\"] = 0.0\n",
    "    elif profile_type == \"reactive\":\n",
    "        load_profile[\"p_specified\"] = 0.0\n",
    "        load_profile[\"q_specified\"] = data_reactive\n",
    "    elif profile_type == \"both\":\n",
    "        load_profile[\"p_specified\"] = data_active\n",
    "        load_profile[\"q_specified\"] = data_reactive\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown profile_type '{profile_type}', must be 'active', 'reactive', or 'both'\")\n",
    "\n",
    "    # start composing the return dict\n",
    "    update_data = {ComponentType.sym_load: load_profile}\n",
    "\n",
    "    # If specified build the transformer update data\n",
    "    if transformer_id >= 0:\n",
    "        # initialize the transformer update array\n",
    "        transformer_update = initialize_array(\n",
    "            DatasetType.update, ComponentType.transformer, (active_batch_profile.shape[0], 1)\n",
    "        )\n",
    "\n",
    "        # assign the transformer ID\n",
    "        transformer_update[\"id\"] = np.array([transformer_id], dtype=int)\n",
    "\n",
    "        # set the tap position for all timestamps\n",
    "        transformer_update[\"tap_pos\"] = tap_pos\n",
    "\n",
    "        update_data[ComponentType.transformer] = transformer_update\n",
    "\n",
    "    return update_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9a662",
   "metadata": {},
   "source": [
    "The power flow is calculated with the input and update data, and if the calculation fails, a detailed error message will be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e26f6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_flow(input_data, update_data):\n",
    "    \"\"\"\n",
    "    Assert validity and run the power flow calculation, returning output_data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert_valid_batch_data(\n",
    "            input_data=input_data, update_data=update_data, calculation_type=CalculationType.power_flow\n",
    "        )\n",
    "        model = PowerGridModel(input_data=input_data)\n",
    "        output_data = model.calculate_power_flow(\n",
    "            update_data=update_data, calculation_method=CalculationMethod.newton_raphson\n",
    "        )\n",
    "    except ValidationException as ex:\n",
    "        # Print each error found\n",
    "        for error in ex.errors:\n",
    "            print(type(error).__name__, error.component, \":\", error.ids)\n",
    "        raise\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2771b",
   "metadata": {},
   "source": [
    "Now, the power flow data can be analyzed with the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9160be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_stats(output_data, batch_profile: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each node in batch_profile.index, compute min/max voltages and corresponding node indices.\n",
    "    Returns a DataFrame with columns: [\"id\", \"Max_voltage\", \"Max_voltage_node\", \"Min_voltage\", \"Min_voltage_node\"].\n",
    "    \"\"\"\n",
    "    ids_node = batch_profile.index\n",
    "    n = len(ids_node)\n",
    "    min_voltage = np.zeros(n)\n",
    "    max_voltage = np.zeros(n)\n",
    "    min_voltage_id = np.zeros(n, dtype=int)\n",
    "    max_voltage_id = np.zeros(n, dtype=int)\n",
    "\n",
    "    u_pu = output_data[ComponentType.node][\"u_pu\"]\n",
    "    # For each timestamp index i, find min/max along the node axis\n",
    "    for i in range(n):\n",
    "        row = u_pu[i, :]\n",
    "        min_idx = row.argmin()\n",
    "        max_idx = row.argmax()\n",
    "        min_voltage[i] = row[min_idx]\n",
    "        max_voltage[i] = row[max_idx]\n",
    "        # +1 to handle 0-based indexing in Python\n",
    "        min_voltage_id[i] = min_idx + 1\n",
    "        max_voltage_id[i] = max_idx + 1\n",
    "\n",
    "    output_node = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids_node,\n",
    "            \"Max_voltage\": max_voltage,\n",
    "            \"Max_voltage_node\": max_voltage_id,\n",
    "            \"Min_voltage\": min_voltage,\n",
    "            \"Min_voltage_node\": min_voltage_id,\n",
    "        }\n",
    "    )\n",
    "    return output_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61bbd335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_line_stats(output_data, batch_profile: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    For each line ID, compute min/max loading with timestamps, and total energy loss.\n",
    "    Returns a DataFrame with columns:\n",
    "      [\"id\", \"Total_loss\", \"Max_loading\", \"Max_loading_timestamp\", \"Min_loading\", \"Min_loading_timestamp\"].\n",
    "    \"\"\"\n",
    "    # find all the unique line IDs\n",
    "    ids = np.unique(output_data[ComponentType.line][\"id\"])\n",
    "    n_lines = len(ids)\n",
    "    min_loading = np.zeros(n_lines)\n",
    "    max_loading = np.zeros(n_lines)\n",
    "    min_loading_timestamp = np.empty(n_lines, dtype=object)\n",
    "    max_loading_timestamp = np.empty(n_lines, dtype=object)\n",
    "\n",
    "    loading = output_data[ComponentType.line][\"loading\"]\n",
    "    timestamps = batch_profile.index\n",
    "\n",
    "    # Compute min/max and find matching timestamp\n",
    "    for i in range(n_lines):\n",
    "        col = loading[:, i]\n",
    "        min_val = col.min()\n",
    "        max_val = col.max()\n",
    "        min_loading[i] = min_val\n",
    "        max_loading[i] = max_val\n",
    "\n",
    "        min_idx = int(np.where(col == min_val)[0][0])\n",
    "        max_idx = int(np.where(col == max_val)[0][0])\n",
    "        min_loading_timestamp[i] = timestamps[min_idx]\n",
    "        max_loading_timestamp[i] = timestamps[max_idx]\n",
    "\n",
    "    # Compute total energy loss with trapezoidal rule:\n",
    "    p_from = output_data[ComponentType.line][\"p_from\"]\n",
    "    p_to = output_data[ComponentType.line][\"p_to\"]\n",
    "    total_loss = np.trapezoid(p_from + p_to, dx=3600 * (10**9), axis=0) / (3.6 * (10**15))\n",
    "\n",
    "    output_line = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"Total_loss\": total_loss,\n",
    "            \"Max_loading\": max_loading,\n",
    "            \"Max_loading_timestamp\": max_loading_timestamp,\n",
    "            \"Min_loading\": min_loading,\n",
    "            \"Min_loading_timestamp\": min_loading_timestamp,\n",
    "        }\n",
    "    )\n",
    "    return output_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1c0bb",
   "metadata": {},
   "source": [
    "The data can be shown using the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a92060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load input data: There are 29 validation errors in data:\n",
      "   1. E\n",
      "   2. r\n",
      "   3. r\n",
      "   4. o\n",
      "   5. r\n",
      "   6.  \n",
      "   7. r\n",
      "   8. e\n",
      "   9. a\n",
      "  10. d\n",
      "  11. i\n",
      "  12. n\n",
      "  13. g\n",
      "  14.  \n",
      "  15. i\n",
      "  16. n\n",
      "  17. p\n",
      "  18. u\n",
      "  19. t\n",
      "  20.  \n",
      "  21. d\n",
      "  22. a\n",
      "  23. t\n",
      "  24. a\n",
      "  25.  \n",
      "  26. f\n",
      "  27. i\n",
      "  28. l\n",
      "  29. e\n"
     ]
    }
   ],
   "source": [
    "def display_results(output_node: pd.DataFrame, output_line: pd.DataFrame):\n",
    "    \"\"\"Display the node and line result tables.\"\"\"\n",
    "    print(\"\\nSelf-made output node table: \")\n",
    "    display(output_node)\n",
    "    print(\"\\nSelf-made line table: \")\n",
    "    display(output_line)\n",
    "\n",
    "\n",
    "def compare_with_expected(timestamp_table_path: str, line_table_path: str):\n",
    "    \"\"\"Read expected output tables and display them for comparison.\"\"\"\n",
    "    try:\n",
    "        expected_output_timestamp = pd.read_parquet(timestamp_table_path)\n",
    "        expected_output_line = pd.read_parquet(line_table_path)\n",
    "    except Exception as e:\n",
    "        raise ValidationException(f\"Error reading expected output files: {e}\")\n",
    "\n",
    "    # print(\"\\nProvided timestamp table: \")\n",
    "    # display(expected_output_timestamp)\n",
    "    # print(\"\\nProvided line table: \")\n",
    "    # display(expected_output_line)\n",
    "\n",
    "\n",
    "def power_flow_results(update_data, batch_profile):\n",
    "    \"\"\"\n",
    "    This function performs an analysis of the power flow results by:\n",
    "      - Asserting and computing power flow\n",
    "      - Calculating node stats\n",
    "      - Calculating line stats (including energy loss)\n",
    "      - Displaying results and comparing with expected tables\n",
    "    \"\"\"\n",
    "    # Compute power flow\n",
    "    try:\n",
    "        output_data = calculate_power_flow(input_data=input_data, update_data=update_data)\n",
    "    except ValidationException:\n",
    "        return\n",
    "\n",
    "    # Node calculation\n",
    "    output_node = calculate_node_stats(output_data, batch_profile)\n",
    "\n",
    "    # Line calculation\n",
    "    output_line = calculate_line_stats(output_data, batch_profile)\n",
    "\n",
    "    display_results(output_node, output_line)\n",
    "\n",
    "    # Compare with expected outputs\n",
    "    compare_with_expected(\n",
    "        \"data/expected_output/output_table_row_per_timestamp.parquet\",\n",
    "        \"data/expected_output/output_table_row_per_line.parquet\",\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Executes all the functions: load data, prepare update_data, and call power_flow_results.\"\"\"\n",
    "    # Load input network data\n",
    "    input_filepath = \"data/input/input_network_data.json\"\n",
    "    try:\n",
    "        global input_data  # so power_flow_results can access it by name as in original code\n",
    "        input_data = load_input_data(input_filepath)\n",
    "    except ValidationException as e:\n",
    "        print(f\"Failed to load input data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load batch profiles\n",
    "    try:\n",
    "        active_power_profile, reactive_power_profile = load_batch_profiles(\n",
    "            \"data/input/active_power_profile.parquet\",\n",
    "            \"data/input/reactive_power_profile.parquet\",\n",
    "        )\n",
    "    except ValidationException as e:\n",
    "        print(f\"Failed to load batch profiles: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prepare update_data for active profile, you can also add reactive now\n",
    "    update_data = prepare_update_data(active_power_profile, reactive_power_profile, profile_type=\"both\")\n",
    "    # Run the analysis\n",
    "    power_flow_results(update_data, active_power_profile)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccbb04d",
   "metadata": {},
   "source": [
    "# **Assignment 3 - developing a power system simulation package**\n",
    "\n",
    "Now that graph processing and power grid model have been implemented, they can be combined to form a functional user package to simulate power systems. The package has 4 functionalities:\n",
    "\n",
    "1. Input data validity check\n",
    "2. EV penetration level\n",
    "3. Optimal tap position\n",
    "4. N-1 calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c78b18d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/input_network_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/input_network_data.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m      2\u001b[39m     data = fp.read()\n\u001b[32m      4\u001b[39m input_data = json_deserialize(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sendt\\OneDrive - TU Eindhoven\\Documents\\Github\\power-system-simulation-Power_Factor\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/input_network_data.json'"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "    \"C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/input_network_data.json\"\n",
    ") as fp:\n",
    "    data = fp.read()\n",
    "\n",
    "input_data = json_deserialize(data)\n",
    "\n",
    "with open(\"C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/meta_data.json\") as fp:\n",
    "    meta = fp.read()\n",
    "\n",
    "meta_data = json.loads(meta)\n",
    "pprint.pprint(meta_data)\n",
    "\n",
    "active_power_profile = pd.read_parquet(\n",
    "    \"C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/active_power_profile.parquet\"\n",
    ")\n",
    "reactive_power_profile = pd.read_parquet(\n",
    "    \"C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/active_power_profile.parquet\"\n",
    ")\n",
    "ev_active_power_profile = pd.read_parquet(\n",
    "    \"C:/Users/Kossyo/Desktop/Уни/Power System computation and simulation/big_network/input/active_power_profile.parquet\"\n",
    ")\n",
    "\n",
    "dtype = {\n",
    "    \"names\": [\n",
    "        \"id\",\n",
    "        \"from_node\",\n",
    "        \"to_node\",\n",
    "        \"from_status\",\n",
    "        \"to_status\",\n",
    "        \"r1\",\n",
    "        \"x1\",\n",
    "        \"c1\",\n",
    "        \"tan1\",\n",
    "        \"r0\",\n",
    "        \"x0\",\n",
    "        \"c0\",\n",
    "        \"tan0\",\n",
    "        \"i_n\",\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(\n",
    "    input_data[ComponentType.line], columns=dtype[\"names\"]\n",
    ")  # get the data for the lines of the grid as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c7975",
   "metadata": {},
   "source": [
    "# **Input data validity check**\n",
    "\n",
    "Check the following validity criteria for the input data. Raise or passthrough relevant errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoreThanOneTransformerOrSource(Exception):\n",
    "    \"Raised when there is more than one transformer or source in meta_data.json\"\n",
    "\n",
    "\n",
    "class InvalidLVIds(Exception):\n",
    "    \"Raised when LV Feeder IDs are not valid line IDs.\"\n",
    "\n",
    "\n",
    "class NonMatchingTransformerLineNodes(Exception):\n",
    "    \"Raised when the lines in the LV Feeder IDs do not have the from_node the same as the to_node of the transformer.\"\n",
    "\n",
    "\n",
    "class NonMatchingTimestamps(Exception):\n",
    "    \"Raised when the timestamps are not matching between the active load profile, reactive load profile, and EV charging profile.\"\n",
    "\n",
    "\n",
    "class InvalidProfileIds(Exception):\n",
    "    \"Raised when the IDs in active load profile and reactive load profile are not matching.\"\n",
    "\n",
    "\n",
    "class InvalidSymloadIds(Exception):\n",
    "    \"Raised when the IDs in active load profile and reactive load profile are not matching the symload IDs.\"\n",
    "\n",
    "\n",
    "class InvalidNumberOfEVProfiles(Exception):\n",
    "    \"raised when the number of EV charging profile is at least the same as the number of sym_load.\"\n",
    "\n",
    "\n",
    "class InvalidLineIds(Exception):\n",
    "    \"Raised when the given Line ID to disconnect is not a valid\"\n",
    "\n",
    "\n",
    "class NonConnnected(Exception):\n",
    "    \"Raised when the given Line ID is not connected at both sides in the base case\"\n",
    "\n",
    "\n",
    "class InvalidCriteria(Exception):\n",
    "    \"Raised when the criteria for optimal tap position is not valid. Use 'Voltage_deviation' or 'Total_loss'.\"\n",
    "\n",
    "\n",
    "def check_source_transformer(meta_data):\n",
    "    if (type(meta_data[\"source\"]) is not int) or (type(meta_data[\"transformer\"]) is not int):\n",
    "        raise MoreThanOneTransformerOrSource(\"LV grid contains more than one source or transformer\")\n",
    "\n",
    "\n",
    "def check_valid_LV_ids(LV_ids, Line_ids):\n",
    "    if not all(item in Line_ids for item in LV_ids):\n",
    "        raise InvalidLVIds(\"LV feeders contain invalid ids\")\n",
    "\n",
    "\n",
    "def check_line_transformer_nodes(lines_from_nodes, transformer_to_node):\n",
    "    if not all(element == transformer_to_node for element in lines_from_nodes):\n",
    "        raise NonMatchingTransformerLineNodes(\n",
    "            \"The lines in the LV Feeder IDs do not have the from_node the same as the to_node of the transformer\"\n",
    "        )\n",
    "\n",
    "\n",
    "def check_timestamps(active_timestamp, reactive_timestamp, ev_timestamp):\n",
    "    if not (active_timestamp.equals(reactive_timestamp) and active_timestamp.equals(ev_timestamp)):\n",
    "        raise NonMatchingTimestamps(\"Timestamps between the active, reactive and ev profiles do not match\")\n",
    "\n",
    "\n",
    "def check_profile_ids(active_ids, reactive_ids):\n",
    "    if not active_ids.equals(reactive_ids):\n",
    "        raise InvalidProfileIds(\"The active and reactive load profile IDs are not matching\")\n",
    "\n",
    "\n",
    "def check_symload_ids(active_ids, reactive_ids, symload_ids):\n",
    "    if not (all(item in symload_ids for item in active_ids) and (all(item in symload_ids for item in reactive_ids))):\n",
    "        raise InvalidSymloadIds(\"The active and reactive load profile IDs are not matching the sym_load IDs\")\n",
    "\n",
    "\n",
    "def check_number_of_ev_profiles(ev_profiles, symload_profiles):\n",
    "    if len(ev_profiles) > len(symload_profiles):\n",
    "        raise InvalidNumberOfEVProfiles(\"The number of EV charging profile is larger than the number of sym_loads\")\n",
    "\n",
    "\n",
    "def check_valid_line_ids(id, Line_ids):\n",
    "    if id not in Line_ids:\n",
    "        raise InvalidLineIds(\"Invalid Line ID\")\n",
    "\n",
    "\n",
    "def check_line_id_connected(fr, to):\n",
    "    if not (fr.item() == 1 and to.item() == 1):\n",
    "        raise NonConnnected(\"Line ID not connected at both sides in the base case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1439a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data_validity_check(input_data, meta_data):\n",
    "\n",
    "    assert_valid_input_data(\n",
    "        input_data=input_data, calculation_type=CalculationType.power_flow\n",
    "    )  # check if input data is valid\n",
    "    check_source_transformer(meta_data)  # check if LV grid has exactly one transformer, and one source.\n",
    "\n",
    "    check_valid_LV_ids(\n",
    "        meta_data[\"lv_feeders\"], input_data[ComponentType.line][\"id\"]\n",
    "    )  # check if All IDs in the LV Feeder IDs are valid line IDs\n",
    "    check_line_transformer_nodes(\n",
    "        df[df[\"id\"].isin(meta_data[\"lv_feeders\"])][\"from_node\"].tolist(),\n",
    "        input_data[ComponentType.transformer][\"to_node\"],\n",
    "    )  # check if all the lines in the LV Feeder IDs have the from_node the same as the to_node of the transformer\n",
    "\n",
    "    transformer_tuple = list(\n",
    "        zip(\n",
    "            input_data[ComponentType.transformer][\"from_node\"].tolist(),\n",
    "            input_data[ComponentType.transformer][\"to_node\"].tolist(),\n",
    "        )\n",
    "    )  # transformer also connects two nodes\n",
    "    line_nodes_id_pairs = (\n",
    "        list(\n",
    "            zip(\n",
    "                input_data[ComponentType.line][\"from_node\"].tolist(), input_data[ComponentType.line][\"to_node\"].tolist()\n",
    "            )\n",
    "        )\n",
    "        + transformer_tuple\n",
    "    )  # add nodes connected by transformer to list of lines connecting nodes\n",
    "    status_list = list(input_data[ComponentType.line][\"to_status\"].tolist()) + list(\n",
    "        input_data[ComponentType.transformer][\"to_status\"].tolist()\n",
    "    )  # add transformer connection status to list of lines' statuses\n",
    "\n",
    "    check_connect(\n",
    "        input_data[ComponentType.node][\"id\"], status_list, line_nodes_id_pairs\n",
    "    )  # check if the grid is fully connected in the initial state\n",
    "    check_cycle(status_list, line_nodes_id_pairs)  # check if the grid has no cycles in the initial state\n",
    "\n",
    "    check_timestamps(\n",
    "        active_power_profile.index, reactive_power_profile.index, ev_active_power_profile.index\n",
    "    )  # checks if timestamps are matching\n",
    "    check_profile_ids(\n",
    "        active_power_profile.columns, reactive_power_profile.columns\n",
    "    )  # checks if number of active and reactive profiles are matching\n",
    "    check_symload_ids(\n",
    "        active_power_profile.columns, reactive_power_profile.columns, input_data[ComponentType.sym_load][\"id\"]\n",
    "    )  # checks if IDs are matching\n",
    "    check_number_of_ev_profiles(\n",
    "        ev_active_power_profile.columns, input_data[ComponentType.sym_load][\"id\"]\n",
    "    )  # checks if number of EV profiles does not exceed number of sym_loads\n",
    "    print(\"Input data is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4e730",
   "metadata": {},
   "source": [
    "Validity check tested on big network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534701f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data is valid!\n"
     ]
    }
   ],
   "source": [
    "input_data_validity_check(input_data, meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8558079",
   "metadata": {},
   "source": [
    "# **EV penetration**\n",
    "\n",
    "Given a (user-provided) input of electrical vehicle (EV) penetration level, i.e. the percentage of houses which has EV charged at home, randomly add EV charging profiles to the houses according to the following creteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c8841",
   "metadata": {},
   "source": [
    "# **Optimal tap position**\n",
    "\n",
    "In this functionality, the user would like to optimize the tap position of the transformer in the LV grid. The user can specifiy which optimization criteria is chosen to determine the optimal tap position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import power_system_simulation.assignment_2 as a2\n",
    "\n",
    "\n",
    "def optimal_tap_position(criteria):\n",
    "    \"\"\"calculates the optimal tap position for the transformer based on the specified criteria\"\"\"\n",
    "    if criteria not in [\"Voltage_deviation\", \"Total_loss\"]:\n",
    "        raise InvalidCriteria(\"Invalid criteria specified. Use 'Voltage_deviation' or 'Total_loss'.\")\n",
    "\n",
    "    transformer_data = input_data[ComponentType.transformer]\n",
    "    tap_min, tap_max = int(transformer_data[\"tap_min\"][0]), int(transformer_data[\"tap_max\"][0])\n",
    "    tap_steps = list(range(tap_max, tap_min + 1))\n",
    "\n",
    "    id = transformer_data[\"id\"]\n",
    "    total_loss = []\n",
    "    total_deviation = []\n",
    "    for tap in tap_steps:\n",
    "        update_data = a2.prepare_update_data(active_power_profile, reactive_power_profile, tap, id, \"both\")\n",
    "        output_data = a2.calculate_power_flow(input_data, update_data)\n",
    "\n",
    "        output_line = a2.calculate_line_stats(output_data, active_power_profile)\n",
    "        output_node = a2.calculate_node_stats(output_data, active_power_profile)\n",
    "        # a2.display_results(output_node, output_line)\n",
    "\n",
    "        total_loss.append(output_line[\"Total_loss\"].sum())\n",
    "\n",
    "        deviation_max = abs(output_node[\"Max_voltage\"] - 1).sum()\n",
    "        deviation_min = abs(output_node[\"Min_voltage\"] - 1).sum()\n",
    "        total_deviation.append(\n",
    "            (deviation_max + deviation_min) / (len(output_node) * 2)\n",
    "        )  # Divide total deviation by number of nodes (max and min) and number of timestamps\n",
    "\n",
    "    if criteria == \"Voltage_deviation\":\n",
    "        optimal_tap = tap_steps[np.argmin(total_deviation)]\n",
    "        display(f\"Optimal tap position: {optimal_tap}, with total deviation: {min(total_deviation)}\")\n",
    "\n",
    "    elif criteria == \"Total_loss\":\n",
    "        optimal_tap = tap_steps[np.argmin(total_loss)]\n",
    "        display(f\"Optimal tap position: {optimal_tap}, with total loss: {min(total_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf22e2",
   "metadata": {},
   "source": [
    "# **N-1 calculation**\n",
    "\n",
    "In this functionality, the user would like to know alternative grid topology when a given line is out of service. The user will provide the Line ID which is going to be out of service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid_line_ids(id, Line_ids):\n",
    "    if id not in Line_ids:\n",
    "        raise InvalidLineIds(\"Invalid Line ID\")\n",
    "\n",
    "\n",
    "def check_line_id_connected(fr, to):\n",
    "    if not (fr.item() == 1 and to.item() == 1):\n",
    "        raise NonConnnected(\"Line ID not connected at both sides in the base case\")\n",
    "\n",
    "\n",
    "def find_alternative_lines(\n",
    "    vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id, id_to_disconnect\n",
    "):\n",
    "    test = GraphProcessor(\n",
    "        vertex_ids, edge_ids, edge_vertex_id_pairs, edge_enabled, source_vertex_id\n",
    "    )  # create a graph from the provided data about nodes and lines\n",
    "    return test.find_alternative_edges(\n",
    "        id_to_disconnect\n",
    "    )  # find the alternative edges to make the graph fully connected, when the line is disconnected\n",
    "\n",
    "\n",
    "def power_flow_calc(active_power_profile, alt_lines_list, line_id_list):\n",
    "\n",
    "    load_profile_active = initialize_array(DatasetType.update, ComponentType.sym_load, active_power_profile.shape)\n",
    "    load_profile_active[\"id\"] = active_power_profile.columns.to_numpy()\n",
    "    load_profile_active[\"p_specified\"] = active_power_profile.to_numpy()\n",
    "    load_profile_active[\"q_specified\"] = 0.0\n",
    "    update_data = {ComponentType.sym_load: load_profile_active}\n",
    "\n",
    "    input_data[ComponentType.line][\"from_status\"][\n",
    "        line_id_list.index(id_to_disconnect)\n",
    "    ] = 0  # disconnect the line that the user wants to disconnect\n",
    "    input_data[ComponentType.line][\"to_status\"][\n",
    "        line_id_list.index(id_to_disconnect)\n",
    "    ] = 0  # disconnect the line that the user wants to disconnect\n",
    "\n",
    "    max_loading_alt = np.zeros(len(alt_lines_list))  # initialize max_loading column values\n",
    "    max_line_alt = np.zeros(len(alt_lines_list))  # initialize max_line_alt column values\n",
    "    max_loading_timestamp_alt = np.zeros(\n",
    "        len(alt_lines_list), dtype=object\n",
    "    )  # initialize max_loading_timestamp column values\n",
    "    for k in range(len(alt_lines_list)):\n",
    "        input_data[ComponentType.line][\"from_status\"][k] = 1\n",
    "        input_data[ComponentType.line][\"to_status\"][k] = 1  # connect the kth alternative line\n",
    "        model = PowerGridModel(input_data=input_data)\n",
    "        result = model.calculate_power_flow(\n",
    "            update_data=update_data, calculation_method=CalculationMethod.newton_raphson\n",
    "        )  # perform the power flow analysis when the kth line is connected\n",
    "        # print(alt_lines_list[k])\n",
    "        # print(result)\n",
    "        ids = np.unique(result[ComponentType.line][\"id\"])\n",
    "        max_loading = np.zeros(len(ids))\n",
    "        max_loading_timestamp = np.zeros(len(ids), dtype=object)\n",
    "        temp_max = -99999999\n",
    "        for i in range(len(ids)):\n",
    "            max_loading[i] = result[ComponentType.line][\"loading\"][:, i].max()\n",
    "            for j in range(len(active_power_profile.index)):\n",
    "                if max_loading[i] == result[ComponentType.line][\"loading\"][j, i]:\n",
    "                    max_loading_timestamp[i] = active_power_profile.index[j]\n",
    "                    if max_loading[i] > temp_max:\n",
    "                        temp_max = max_loading[i]\n",
    "                        max_loading_alt[k] = max_loading[i]\n",
    "                        max_line_alt[k] = result[ComponentType.line][\"id\"][j, i]\n",
    "                        max_loading_timestamp_alt[k] = active_power_profile.index[j]\n",
    "                        # print(active_power_profile.index[j])\n",
    "        input_data[ComponentType.line][\"from_status\"][k] = 0\n",
    "        input_data[ComponentType.line][\"to_status\"][k] = 0  # disconnect kth line\n",
    "\n",
    "    input_data[ComponentType.line][\"from_status\"][\n",
    "        line_id_list.index(id_to_disconnect)\n",
    "    ] = 1  # reconnect the line that the user wants to disconnect\n",
    "    input_data[ComponentType.line][\"to_status\"][\n",
    "        line_id_list.index(id_to_disconnect)\n",
    "    ] = 1  # reconnect the line that the user wants to disconnect\n",
    "\n",
    "    output_data = pd.DataFrame()  # generate specified table from assignment 3\n",
    "    output_data[\"Alternative line ID\"] = alt_lines_list\n",
    "    output_data[\"Max_loading\"] = max_loading_alt\n",
    "    output_data[\"Max_line_id\"] = max_line_alt\n",
    "    output_data[\"Max_timestamp\"] = max_loading_timestamp_alt\n",
    "    display(output_data)\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_minus_one_calculation(id_to_disconnect):\n",
    "\n",
    "    check_valid_line_ids(id_to_disconnect, input_data[ComponentType.line][\"id\"])  # check if ID is valid\n",
    "    check_line_id_connected(\n",
    "        df[df[\"id\"] == id_to_disconnect][\"from_status\"], df[df[\"id\"] == id_to_disconnect][\"to_status\"]\n",
    "    )  # check if line with selected ID is connected\n",
    "\n",
    "    transformer_tuple = list(\n",
    "        zip(\n",
    "            input_data[ComponentType.transformer][\"from_node\"].tolist(),\n",
    "            input_data[ComponentType.transformer][\"to_node\"].tolist(),\n",
    "        )\n",
    "    )  # transformer also connects two nodes\n",
    "    line_nodes_id_pairs = (\n",
    "        list(\n",
    "            zip(\n",
    "                input_data[ComponentType.line][\"from_node\"].tolist(), input_data[ComponentType.line][\"to_node\"].tolist()\n",
    "            )\n",
    "        )\n",
    "        + transformer_tuple\n",
    "    )  # add nodes connected by transformer to list of lines connecting nodes\n",
    "    status_list = list(df[\"to_status\"].tolist()) + list(\n",
    "        input_data[ComponentType.transformer][\"to_status\"].tolist()\n",
    "    )  # add transformer connection status to list of lines' statuses\n",
    "    line_id_list = df[\"id\"].tolist()\n",
    "    new_id = (df[\"id\"].iloc[-1] + 1).tolist()\n",
    "    line_id_list.append(new_id)  # add another line id to mimic the transformer connection\n",
    "    alt_lines_list = find_alternative_lines(\n",
    "        input_data[ComponentType.node][\"id\"].tolist(),\n",
    "        line_id_list,\n",
    "        line_nodes_id_pairs,\n",
    "        status_list,\n",
    "        meta_data[\"mv_source_node\"],\n",
    "        id_to_disconnect,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"To make the grid fully connected, the following lines need to be connected: {alt_lines_list}\"\n",
    "    )  # find alternative currently disconnected lines to make the grid fully connected\n",
    "\n",
    "    power_flow_calc(active_power_profile, alt_lines_list, line_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c41d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make the grid fully connected, the following lines need to be connected: [2010]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alternative line ID</th>\n",
       "      <th>Max_loading</th>\n",
       "      <th>Max_line_id</th>\n",
       "      <th>Max_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.073859</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>2025-11-05 06:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alternative line ID  Max_loading  Max_line_id       Max_timestamp\n",
       "0                 2010     0.073859       1906.0 2025-11-05 06:45:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_to_disconnect = 2002  # returns [2010] list and a table of 1 row\n",
    "N_minus_one_calculation(id_to_disconnect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
